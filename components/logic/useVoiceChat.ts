import { useCallback, useEffect, useRef } from "react";
import { useStreamingAvatarContext } from "./context";

/**
 * üéß Voice Chat HeyGen (SDK v2+)
 * - Ne PAS passer de MediaStream au SDK (il g√®re le micro en interne)
 * - On demande quand m√™me l'autorisation micro pour fiabiliser l'exp√©rience
 * - Gestion compl√®te des √©tats: isMuted, isVoiceChatActive, loaders
 */
export const useVoiceChat = () => {
  const {
    avatarRef,
    isMuted,
    setIsMuted,
    isVoiceChatActive,
    setIsVoiceChatActive,
    isVoiceChatLoading,
    setIsVoiceChatLoading,
  } = useStreamingAvatarContext();

  const micStreamRef = useRef<MediaStream | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);

  /** üéôÔ∏è Demande l‚Äôacc√®s micro (d√©clenche le prompt navigateur) */
  const requestMicAccess = async (): Promise<MediaStream | null> => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      console.log("üé§ Micro autoris√©");
      return stream;
    } catch (err) {
      console.error("üö´ Acc√®s micro refus√© :", err);
      alert("Veuillez autoriser le micro pour parler √† l‚Äôavatar.");
      return null;
    }
  };

  /** üöÄ D√©marre le Voice Chat (sans stream) */
  const startVoiceChat = useCallback(
    async (isInputAudioMuted?: boolean) => {
      if (!avatarRef.current) {
        console.warn("‚ö†Ô∏è Avatar non initialis√© pour le voice chat");
        return;
      }
      if (isVoiceChatActive) {
        // d√©j√† actif ‚Üí on aligne juste l'√©tat mute si demand√©
        if (typeof isInputAudioMuted === "boolean") {
          if (isInputAudioMuted) {
            avatarRef.current.muteInputAudio?.();
          } else {
            avatarRef.current.unmuteInputAudio?.();
          }
          setIsMuted(!!isInputAudioMuted);
        }
        return;
      }

      setIsVoiceChatLoading(true);
      try {
        // ‚öôÔ∏è AudioContext (certains navigateurs exigent un contexte actif)
        if (!audioContextRef.current) {
          audioContextRef.current = new AudioContext();
        } else if (audioContextRef.current.state === "suspended") {
          await audioContextRef.current.resume();
        }

        // üéôÔ∏è Demander l'autorisation micro (pour √©viter les surprises)
        const micStream = await requestMicAccess();
        if (!micStream) throw new Error("Micro introuvable ou refus√©");
        // On n'utilise PAS le stream avec le SDK, on l'arr√™te apr√®s autorisation
        micStreamRef.current = micStream;

        // üîó Lancer le voice chat (SANS 'stream')
        const startOptions: { isInputAudioMuted?: boolean } = {};
        if (typeof isInputAudioMuted === "boolean") {
          startOptions.isInputAudioMuted = isInputAudioMuted;
        }

        await avatarRef.current.startVoiceChat(startOptions);

        // On peut couper le flux utilis√© pour l'autorisation : le SDK a son propre flux
        micStreamRef.current.getTracks().forEach((t) => t.stop());
        micStreamRef.current = null;

        console.log("‚úÖ VoiceChat connect√© avec succ√®s");
        setIsVoiceChatActive(true);
        setIsMuted(!!isInputAudioMuted);

        // (optionnel) events si le SDK en expose pour le voice chat
        avatarRef.current.on?.("voice_chat_reconnected" as any, () => {
          console.log("üîÑ Reconnexion audio r√©ussie");
          setIsVoiceChatActive(true);
        });
        avatarRef.current.on?.("voice_chat_disconnected" as any, () => {
          console.warn("‚ö†Ô∏è VoiceChat d√©connect√©");
          setIsVoiceChatActive(false);
        });
      } catch (err) {
        console.error("‚ùå Erreur lors du d√©marrage VoiceChat:", err);
        setIsVoiceChatActive(false);
      } finally {
        setIsVoiceChatLoading(false);
      }
    },
    [avatarRef, isVoiceChatActive, setIsMuted, setIsVoiceChatActive, setIsVoiceChatLoading],
  );

  /** üõë Arr√™te le Voice Chat et coupe le micro */
  const stopVoiceChat = useCallback(() => {
    if (!avatarRef.current) return;

    try {
      console.log("üõë Arr√™t du VoiceChat");
      avatarRef.current.closeVoiceChat?.();
    } catch (err) {
      console.error("‚ö†Ô∏è Erreur √† l‚Äôarr√™t du VoiceChat:", err);
    }

    // stoppe tout flux temporaire si encore pr√©sent
    micStreamRef.current?.getTracks().forEach((t) => t.stop());
    micStreamRef.current = null;

    setIsVoiceChatActive(false);
    setIsMuted(true);
  }, [avatarRef, setIsMuted, setIsVoiceChatActive]);

  /** üîá Mute audio input (contr√¥le SDK) */
  const muteInputAudio = useCallback(() => {
    if (!avatarRef.current) return;
    avatarRef.current.muteInputAudio?.();
    setIsMuted(true);
    console.log("üîá Micro coup√©");
  }, [avatarRef, setIsMuted]);

  /** üîä Unmute audio input (contr√¥le SDK) */
  const unmuteInputAudio = useCallback(() => {
    if (!avatarRef.current) return;
    avatarRef.current.unmuteInputAudio?.();
    setIsMuted(false);
    console.log("üé§ Micro r√©activ√©");
  }, [avatarRef, setIsMuted]);

  /** ‚ôªÔ∏è Nettoyage √† la fermeture */
  useEffect(() => {
    return () => {
      try {
        stopVoiceChat();
      } finally {
        micStreamRef.current?.getTracks().forEach((t) => t.stop());
        micStreamRef.current = null;
        audioContextRef.current?.close();
        audioContextRef.current = null;
      }
    };
  }, [stopVoiceChat]);

  return {
    startVoiceChat,
    stopVoiceChat,
    muteInputAudio,
    unmuteInputAudio,
    isMuted,
    isVoiceChatActive,
    isVoiceChatLoading,
  };
};
